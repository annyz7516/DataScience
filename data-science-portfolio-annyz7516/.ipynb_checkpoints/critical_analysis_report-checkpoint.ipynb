{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390acce0",
   "metadata": {},
   "source": [
    "## Critical Analysis Report\n",
    "name: Y Z\n",
    "\n",
    "ID: 00000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1918beef",
   "metadata": {},
   "source": [
    "### Issue 1. \"We note that the \"not.fully.paid\" feature has missed a record. We can repair it by using the average value of this feature to replace the missing value.'\"\n",
    "When dealing with missing data, we have two major options: imputation or remove data.\n",
    "- In these case, there is only one data of one feature missing, the portion of missing data is low.\n",
    "- This feature is a categorical variable\n",
    "In this case, a common way is to drop the row that has missing record. The way to replace missing value by a\n",
    "median or average is normally for continuous variables.\n",
    "\n",
    "#### Solutionï¼šdrop the raw(s) with missing record to remove outliers\n",
    "data = Data.dropna()\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8ec136",
   "metadata": {},
   "source": [
    "### Issue 2. \"If 'int.rate' is negative it will be replaced by the median value of 'int.rate'. If 'days.with.cr.line' is greater than 36,500 days (or 100 years), it will be replaced by the median value of 'days.with.cr.line'\"\n",
    "Some outliers that we can use common sense to find indicate a mistake in data collection.\n",
    "If we know that it's wrong and we have a lot of data, it is safe to drop values that are ourliers.\n",
    "\n",
    "#### Solution: drop the raw(s) with outliers\n",
    "data_clean = data.drop(Data.index[(Data['int.rate']<=0) & (data['days.with.cr.line'] > 36500)])                    \n",
    "data_clean.info    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a294e96",
   "metadata": {},
   "source": [
    "### Issue 3. \"From the heatmaps, there are different correlations between each feature and 'credit.policy'.only reserve features that have positive correlations by removing all features that are negatively correlated with labeled feature.\"\n",
    "- Negative correlation is a relationship between two variables in which one variable increases as the other decreases, and vice versa. \n",
    "- A perfect negative correlation is represented by the value -1.0, while a 0 indicates no correlation.\n",
    "- It is not right to decide whether a correlation is useful or useless by observing correlation positive or negative. \n",
    "- The further away correlation is from zero, the stronger the relationship between the two variables.\n",
    "\n",
    "#### Solution: do not drop the features that have negative correlation with 'credit.policy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1793e8",
   "metadata": {},
   "source": [
    "### Issue 4. \"Sort all records by an ascending order of 'credit.policy'. \"\n",
    "After observing the dataset, we found that all records are sorted in descending order.\n",
    "Because numbers of credit.policy with value=1 is much higher than that with value=0,\n",
    "it is a good idea to contain all data with credit.policy value = 0 in train dataset.\n",
    "Sorting by ascending order is the solution.\n",
    "\n",
    "The problem is the following piece of coding can not successfully change the order\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "New_Data.sort_values(by=['credit.policy'])\n",
    "\n",
    "#### solution\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "New_Data.sort_values(by=['credit.policy'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2443e",
   "metadata": {},
   "source": [
    "### Issue 5. Data normalisation is following Dataset train_test_split\n",
    "The goal of data normalisation is to transform features in a dataset to be on a similar scale.\n",
    "\n",
    "According to sklearn documentation:\n",
    "- fit(X[, y, sample_weight]): Compute the mean and std to be used for later scaling.\n",
    "- fit_transform(X[, y]): Fit to data, then transform it.\n",
    "- transform(X[, copy]): Perform standardization by centering and scaling.\n",
    "    \n",
    "Here we should fit with train dataset for later scaling, then transform both train dataset and test dataset with the same scaler.\n",
    "\n",
    "#### solution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "obje_ss=StandardScaler()\n",
    "\n",
    "x_ex1_train=obje_ss.fit_transform(x_ex1_train)\n",
    "\n",
    "x_ex1_test=obje_ss.transform(x_ex1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979c97d",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________\n",
    "\n",
    "Please check [critical_analysis_revised](http://localhost:8888/notebooks/DSWorkshop/data-science-portfolio-annyz7516/critical_analysis_revised.ipynb) for complete solution with \n",
    "\n",
    "Train success rate: % 99.86078886310905\n",
    "\n",
    "Test success rate: % 99.79101358411702\n",
    "\n",
    "for decision tree model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
